\documentclass[letterpaper,hyper]{THG_RFC}

% Fonts
\usepackage{ifluatex}
\ifluatex
  % Font
  \usepackage{luatextra, xunicode, unicode-math}
  \defaultfontfeatures{Ligatures=TeX}
  \setmainfont[
    UprightFont={*},
    BoldFont={* Bold},
    ItalicFont={* Italic},
    BoldItalicFont={* Bold Italic},
    SlantedFont={* Italic},
    BoldSlantedFont={* Bold Italic}
  ]{Calibri}
  \setsansfont[
    UprightFont={*},
    BoldFont={* Bold},
    ItalicFont={* Italic},
    BoldItalicFont={* Bold Italic}
  ]{Calibri}
  \setmathfont[math-style=ISO, bold-style=ISO]{Asana Math}
  \setmonofont[
    UprightFont={*},
    BoldFont={* Bold},
    ItalicFont={* Italic},
    BoldItalicFont={* Bold Italic}
  ]{Courier New}
\else
  \usepackage{lmodern} % times / lmodern / mathpazo / palatino
  \usepackage[scaled=.95]{helvet}
  \usepackage{courier}
\fi

% Path to figures, plots
\graphicspath{{./pics/}{./plots/}}

%% TikZ
\usepackage{tikz}
\usetikzlibrary{patterns, shapes, decorations.pathreplacing}
\usepackage{gnuplot-lua-tikz}

% SI units
%\usepackage{textcomp}
\usepackage[binary-units]{siunitx}
\sisetup{per-mode = symbol}
\providecommand{\gbps}[1]{\SI{#1}{\giga\byte\per\second}}
\providecommand{\mbps}[1]{\SI{#1}{\mega\byte\per\second}}
\providecommand{\gb}[1]{\SI{#1}{\giga\byte}}
\providecommand{\mb}[1]{\SI{#1}{\mega\byte}}
\providecommand{\kb}[1]{\SI{#1}{\kilo\byte}}


% Code snippets
\usepackage{listings}
\def\lstsetc{\lstset{language=C,
  numbers=left,
  xleftmargin=20pt,
  numberstyle=\tiny\color{gray},
  stepnumber=1,
  showspaces=false, 
  showstringspaces=false,
  breaklines=true,
  basicstyle=\footnotesize\ttfamily,
  stringstyle=\itshape,
  commentstyle=\itshape\bfseries,
  morekeywords={main, size_t, malloc, free,
    hsize_t, hid_t, herr_t, H5X_class_t, H5X_type_t, H5Q_combine_op_t,
    H5Q_type_t, H5Q_match_op_t,
    H5Fcreate, H5Fclose,
    H5Dcreate, H5Dopen, H5Dclose, H5Dwrite, H5Dread, H5Dquery,
    H5Qcreate, H5Qclose, H5Qcombine, H5Qget_type, H5Qget_match_op,
    H5Qget_components, H5Qget_combine_op, H5Qencode, H5Qdecode,
    H5Screate_simple, H5Sclose, H5Sget_select_npoints,
    H5Xregister, H5Xunregister, H5Xcreate, H5Xremove, H5Xget_count
    }
  }
}

% Title, author, etc
\title{Data Analysis Extensions}
\author{Jerome Soumagne}
\author{Quincey Koziol}
\date{July 17, 2014}
\rfcversion{2014-07-17.v1}
\revision{July 17, 2014}{Version 1 circulated for comment within The HDF Group.}

%% Start the document
\begin{document}

%% Add Attribute reference RFC

%% Title
\maketitle

%% Abstract
\begin{abstract}
Accessing and retrieving data from an HDF5 container can be a time consuming
process, particularly so when data is very large. To enable,
ease and accelerate the process of querying data, we introduce in this RFC data
analysis extensions to query, select and index data.
\end{abstract}

\section{Introduction}
Data Analysis Extensions to the HDF5 API and data model can enable application
developers to create complex and high-performance queries on both metadata and
data elements within an HDF5 container and retrieve the results of applying
those query operations to an HDF5 container.
Support for data analysis operations on HDF5 containers can be defined via:
\begin{itemize}
\item New \textit{query} object\footnote{Query and view objects are
\textit{in-memory} objects, which therefore do not modify the content of the
container.\label{fn:object}} and API routines, enabling the construction of
query requests for execution on HDF5 containers;
\item New \textit{view} object\footref{fn:object} and API routines, which apply a query to an HDF5
container and return a set of references into the container that fulfills
the query criteria;
\item New \textit{index} object and API routines, which allows the creation of
indices on the contents of HDF5 containers, to improve query performance.
\end{itemize}

\section{Query Objects}
Query objects are the foundation of the data analysis operations in HDF5 and
can be built up from simple components in a programmatic way to create complex
operations using \textit{Boolean} operations. The current API is presented
below:

{\lstsetc
\begin{lstlisting}
hid_t  H5Qcreate(H5Q_type_t query_type, H5Q_match_op_t match_op, ...);
herr_t H5Qclose(hid_t query_id);

hid_t  H5Qcombine(hid_t query1_id, H5Q_combine_op_t combine_op, hid_t query2_id);

herr_t H5Qget_type(hid_t query_id, H5Q_type_t *query_type);
herr_t H5Qget_match_op(hid_t query_id, H5Q_match_op_t *match_op);
herr_t H5Qget_components(hid_t query_id, hid_t *subquery1_id, hid_t *subquery2_id);
herr_t H5Qget_combine_op(hid_t query_id, H5Q_combine_op_t *op_type);

herr_t H5Qencode(hid_t query_id, void *buf, size_t *nalloc);
hid_t  H5Qdecode(const void *buf);
\end{lstlisting}
}

The core query API is composed of
two routines: \texttt{H5Qcreate} and \texttt{H5Qcombine}. \texttt{H5Qcreate}
creates new queries, by specifying an aspect of an HDF5 container, such as
data elements, link names, attribute names, etc., a match operator, such as
$=$, $\neq$, $\leq$, $\geq$, and a value for the match operator.
Created query objects can be serialized and deserialized using \texttt{H5Qencode}
and \texttt{H5Qdecode} routines\footnote{Useful if the query needs to be sent
through the network}, and their content can be retrieved using the corresponding
accessor routines.
\texttt{H5Qcombine} combines two query objects into a
new query object, using Boolean operators such as $AND(\land)$ and $OR(\lor)$.
Queries created with \texttt{H5Qcombine} can be used as input to further
calls to \texttt{H5Qcombine}, creating more complex queries.

For example, a single call to \texttt{H5Qcreate} could create a query object
that would match data elements in any dataset within the container that are
equal to the value $17$. Another call to \texttt{H5Qcreate} could create a
query object that would match link names equal to $Pressure$.
Calling \texttt{H5Qcombine} with the $\land$ operator and those two query
objects would create a new query object that matched elements equal to $17$
in HDF5 datasets with link names equal to $Pressure$.
Creating the data analysis extensions to HDF5 using a \textit{programmatic
interface} for defining queries avoids defining a text-based query language
as a core component of the data analysis interface, and is more in keeping with
the design and level of abstraction of the HDF5 API.
The HDF5 data model is more complex than traditional database tables and a
simpler query model would likely not be able to express the kinds of queries
needed to extract the full set of components of an HDF5 container. A text-based
query language (or GUI) could certainly be built on top of the query API
defined here to provide a more user-friendly (as opposed to
\textit{developer-friendly}) query syntax like $Pressure = 17$. However, we
regard this as out-of-scope for now.

\TableRef{tab:querycomb} describes the result types for atomic queries and
combining queries of different types. Query results of $None$ type are rejected
when \texttt{H5Qcombine} is called, causing it to return failure\footnote{Query
results of $None$ type may be implemented with another result type in the
future, once experience with the query framework is acquired and a meaningful
grammar for those results are defined.}.

\begin{table}[ht]\footnotesize
\caption{Query combinations and associated result type.}
\label{tab:querycomb}
\begin{tabular}{ll} \toprule
Query &
Result Type\\ \midrule
\texttt{H5Q\_TYPE\_DATA\_ELEM} & $Dataset~Element$ \\
\texttt{H5Q\_TYPE\_ATTR\_VALUE} & $Attribute$ \\
\texttt{H5Q\_TYPE\_ATTR\_NAME} & $Object$ \\
\texttt{H5Q\_TYPE\_LINK\_NAME} & $Object$ \\
$Dataset~Element \land Dataset~Element$ & $Dataset~Element$ \\
$Dataset~Element \land Attribute$ & $None$ \\
$Dataset~Element \land Object$ & $Dataset~Element$ \\
$Attribute \land Attribute$ & $Attribute$ \\
$Attribute \land Object$ & $Attribute$ \\
$Object \land Object$ & $Object$ \\
$Dataset~Element \lor Dataset~Element$ & $Dataset~Element$ \\
$Dataset~Element \lor Attribute$ & $Combination$ \\
$Dataset~Element \lor Object$ & $Combination$ \\
$Dataset~Element \lor Combination$ & $Combination$ \\
$Attribute \lor Attribute$ & $Attribute$ \\
$Attribute \lor Object$ & $Combination$ \\
$Attribute \lor Combination$ & $Combination$ \\
$Object \lor Object$ & $Object$ \\
$Object \lor Combination$ & $Combination$ \\
$Combination \lor Combination$ & $Combination$ \\
$Combination \land Dataset~Element$ & $None$ \\
$Combination \land Attribute$ & $None$ \\
$Combination \land Object$ & $None$ \\
$Combination \land Combination$ & $None$ \\
\bottomrule
\end{tabular}
\end{table}

\section{View Objects}
Applying a query to an HDF5 container creates an HDF5 view object. HDF5 view
objects are runtime, in-memory objects (i.e., not stored in a container) that
consist of read-only references into the contents of the HDF5 container that
the query was applied to. The current API is presented below:

{\lstsetc
\begin{lstlisting}
hid_t H5Vcreate(hid_t loc_id, hid_t query_id, hid_t vcpl_id);
herr_t H5Vclose(hid_t view_id);
herr_t H5Vget_location(hid_t view_id, hid_t *loc_id);
herr_t H5Vget_query(hid_t view_id, hid_t *query_id);
herr_t H5Vget_counts(hid_t view_id, hsize_t *attr_count, hsize_t *obj_count,
  hsize_t *elem_region_count);
herr_t H5Vget_attrs(hid_t view_id, hsize_t start, hsize_t count, hid_t attr_id[]);
herr_t H5Vget_objs(hid_t view_id, hsize_t start, hsize_t count, hid_t obj_id[]);
herr_t H5Vget_elem_regions(hid_t view_id, hsize_t start, hsize_t count,
  hid_t dataset_id[], hid_t dataspace_id[]);
\end{lstlisting}
}

View objects are created with \texttt{H5Vcreate}, which applies a query to an
HDF5 container, group hierarchy, or individual object and produces the view
object as a result. The attributes, objects, and/or data elements referenced
within a view can be retrieved by further API calls. Note that currently,
attribute references are not available, this feature will be added in order to
support views that contain attribute references\footnote{See RFC XXXX for attribute references.}.

\begin{figure}
\input{pics/hdf5_view1}
\caption{HDF5 container example.}
\label{fig:hdf5_view1}
\end{figure}

For example, starting with the HDF5 container described in \FigureRef{fig:hdf5_view1},
applying the $link\_name = Pressure$ query (described above) would result
in the view shown in \FigureRef{fig:hdf5_view2}, with the
underlying container grayed out and the view highlighted in blue.

\begin{figure}
\input{pics/hdf5_view2}
\caption{HDF5 container example with query $link\_name = Pressure$ applied.}
\label{fig:hdf5_view2}
\end{figure}

Alternatively, applying the $data\_element = 17$ query (described above) would
result in the view shown in \FigureRef{fig:hdf5_view3}, with the
underlying container greyed out and the view highlighted in blue.

\begin{figure}
\input{pics/hdf5_view3}
\caption{HDF5 container example with query $data\_element = 17$ applied.}
\label{fig:hdf5_view3}
\end{figure}

Finally, applying the combined $(link\_name = Pressure)\land(data\_element = 17)$
query (described above) would result in the view shown in
\FigureRef{fig:hdf5_view4}, with the underlying container
greyed out and the view highlighted in blue.

\begin{figure}
\input{pics/hdf5_view4}
\caption{HDF5 container example with query $(link\_name = Pressure)\land(data\_element = 17)$ applied.}
\label{fig:hdf5_view4}
\end{figure}

Views can be thought of as containing a set of HDF5 references (object,
dataset region or attribute references) to components of the underlying
container, retaining the context of the original container. For example, the
view containing the results of the $(link\_name = Pressure)\land(data\_element = 17)$
query will contain three dataset region references, which
can be retrieved from the view object and probed for the dataset and selection
containing the elements that match the query with the existing \texttt{H5Rdereference}
and \texttt{H5Rget\_region} API calls. Note that selections returned from a region
reference retain the underlying dataset's dimensionality and coordinates---they
are not \textit{flattened} into a 1-D series of elements. The selection returned
from a region reference can also be applied to a different dataset in the container,
allowing a query on pressure values to be used to extract temperature values,
for example.

\section{Index Objects}
The final component of the data analysis extensions to HDF5 is the index object.
Index objects are designed to accelerate creation of view objects from
frequently occurring query operations.
For example, if the $(link\_name = Pressure)\land(data\_element = 17)$ query
(described above) is going to be frequently executed on the container, indices
could be created in that container, which would speed up the creation of views
when querying for link names and for data element values. Indices created for
accelerating the $link\_name = Pressure$ or $data\_element = 17$ queries
would also improve view creation for the more complex
$(link\_name = Pressure)\land(data\_element = 17)$ query.

Although creating indices for metadata components of queries, such as link or
attribute names, is possible, we focus on index creation for dataset elements,
as they represent the largest volume of data in typical HPC application usage of
HDF5. Queries with metadata components execute properly,
\todo{Must write about metadata indexing} but are not able to be accelerated
with an index currently.

The indexing API works in conjunction with the view API. When an \texttt{H5Vcreate}
call is made for a group or dataset, an index attached to any dataset queried
for element value ranges will be used to speed up the query process and return
a dataspace selection to the library for later use.

There are different techniques for creating data element indices, and the most
efficient method will vary depending on the type of the data that is to be
indexed, its layout, etc. We therefore define a new interface for the HDF5
library that uses a plugin mechanism.

\subsection{Indexing Interface and Plugins}

A new HDF5 interface is defined for adding third-party indexing plugins,
such as FastBit~\cite{Wu05}, ALACRITY~\cite{alacrity13}, etc.
The interface provides indexing plugins with efficient access to the contents of
the container for both the creation and the maintenance of indices. In addition,
the interface allows third-party plugins to create private data structures
within the container for storing the contents of the index.
The current API as well as the plugin interface are presented below:

{\lstsetc
\begin{lstlisting}
herr_t H5Xregister(const H5X_class_t *idx_class);
herr_t H5Xunregister(unsigned plugin_id);

herr_t H5Xcreate(hid_t file_id, unsigned plugin_id, hid_t scope_id,
        hid_t xcpl_id);
herr_t H5Xremove(hid_t file_id, unsigned plugin_id, hid_t scope_id);
herr_t H5Xget_count(hid_t scope_id, hsize_t *idx_count);

typedef struct {
    unsigned version;     /* Version number of the index plugin class struct */
                          /* (Should always be set to H5X_CLASS_VERSION, which
                           *  may vary between releases of HDF5 library) */
    unsigned id;          /* Index ID (assigned by The HDF Group, for now) */
    const char *idx_name; /* Index name (for debugging only, currently) */
    H5X_type_t type;      /* Type of data indexed by this plugin */

    /* Callbacks, described above */
    void *(*create)(hid_t file_id, hid_t dataset_id, hid_t xcpl_id,
        hid_t xapl_id, size_t *metadata_size, void **metadata);
    herr_t (*remove)(hid_t file_id, hid_t dataset_id, size_t metadata_size,
        void *metadata);
    herr_t (*copy)(hid_t file1_id, hid_t file2_id, hid_t dataset1_id,
        hid_t dataset2_id, hid_t xcpl_id, hid_t xapl_id,
        size_t metadata_size, void *metadata);
    void *(*open)(hid_t file_id, hid_t dataset_id, hid_t xapl_id,
        size_t metadata_size, void *metadata);
    herr_t (*close)(void *idx_handle);
    herr_t (*pre_update)(void *idx_handle, hid_t dataspace_id, hid_t xxpl_id);
    herr_t (*post_update)(void *idx_handle, const void *buf, hid_t dataspace_id,
            hid_t xxpl_id);
    herr_t (*query)(void *idx_handle, hid_t query_id, hid_t xxpl_id,
        hid_t *dataspace_id);
    herr_t (*refresh)(void *idx_handle, size_t *metadata_size, void **metadata);
    herr_t (*get_size)(void *idx_handle, size_t *idx_size);
} H5X_class_t;
\end{lstlisting}
}

Index objects are stored in the HDF5 container that they apply to, but are not
visible in the container's group hierarchy\footnote{Plugin developers, note that
the HDF5 library's existing anonymous dataset and group creation calls can be
used to create objects in HDF5 files that are not visible in the container's
group hierarchy.}.
Instead, index objects are part of the metadata for the file itself. New index
objects are created by passing an H5 container to be indexed and the index
plugin ID to the \texttt{H5Xcreate} call.
Alternatively an index may be created at the same time as a dataset gets created
by passing a property to the dataset creation property list.
Index information (such as plugin ID and index metadata) is stored at index
creation time\footnote{This therefore introduces a file format change},
and when the user later calls \texttt{H5Dopen}, the plugin open
callback will retrieve this stored information and make use of the corresponding
index plugin for all subsequent operations. Similarly, calling \texttt{H5Dclose}
will call the plugin index close callback and close the objects used to store
the index data.

\begin{figure}
\input{pics/hdf5_index}
\caption{Index information (plugin ID and metadata) is stored along the object header.}
\label{fig:hdf5_index}
\end{figure}

When a call to \texttt{H5Dwrite} is made, the index plugin \texttt{pre\_update} and
\texttt{post\_update} callbacks will be triggered, allowing efficient index
update by first telling the index plugin the region that is going to be updated
with new data, and then realizing the actual index update, after the dataset
write has completed. This allows various optimization to be made, depending on
the data selection passed and the index plugin used. For example, a plugin could
store the region and defer the actual index update until the dataset is closed,
hence saving repeated index computation/update calls.

When a call to \texttt{H5Vcreate} is made, the index plugin query callback will be
invoked to create a selection of elements in the dataset that match the query
parameters. Applications can also use the new \texttt{H5Dquery} routine to directly
execute a query on a dataset (accelerated by any index defined on the dataset),
retrieving the selection within that dataset that matches the query.

Because the amount of space taken by the index cannot be directly retrieved by
the user since the datasets storing the indices are known only by the plugin
itself, the \texttt{get\_size} callback can query the amount of space that
the index takes in the file.

\todo{Add something about parallel queries / multi dataset serial / multiquery paragraph}

\subsubsection{Current and future plugins}

Current implementations for FastBit and ALACRITY index packages are already
present, as well as a brute force indexing plugin. Early performance results
are presented in \FigureRef{fig:indexing_perf}.

\begin{figure}
\input{plots/indexing_perf}
\caption{Indexing performance.}
\label{fig:indexing_perf}
\end{figure}

In the future more plugins
will be added, with and without external dependency (e.g., pytable indexing,
bitmap indexing).
To satisfy that need, dynamic plugin loading and registration will be supported,
allowing external libraries to plug to the current interface.

\subsection{Limitations}

There are some existing limitations in the use of indices in the current
implementation: FastBit and ALACRITY do not support incremental updates,
an index is a shared resource for a dataset. Taken together, these conspire to
put limits on application updates to datasets with indices.
Additionally, because FastBit and Alacrity don't allow incremental updates to
an index, each modification to an existing index forces the index to be entirely
rebuilt. The limitation in FastBit and ALACRITY will need to be addressed in
the base packages' implementation, so that they can make incremental updates
to their index information.

\subsubsection*{Questions}
Some questions are still open regarding the handling on indices:
\begin{itemize}
\item How could an index be built and queried in parallel for a dataset that already exists?
\item How to handle index updates when the specified index plugin is not available?
(In traditional databases, stored procedures are saved with the data and
therefore available at any time, but that is not the case here)
We could mark the index as out of date and rebuild the index when the plugin is
available again.
\item How to handle index queries when the specified index plugin is not
available? We could fallback to another plugin and do a brute force query on the data.
\end{itemize}

\section{HDF5 and tools}

Existing HDF5 tools must be compatible and take into account the existence of
indices in the file if there are any. For reference, the following behavior for
the tools is given:

\begin{itemize}
\item h5copy: copy
\item h5dump: report index information
\item h5ls: report index information
\item h5diff: ignore index information?
\item h5repack: copy index information / or generate index
\item h5edit: ignore index information?
\item h5toh4: ignore index information?
\item h5import: ignore index information
\end{itemize}

Additional tools for indexing data and answering queries will also be added
in the future.

\section{Usage Example}

In the following example, we show how one can make use of the query and
indexing interface to get a dataspace selection within a dataset,
for simplicity we first create a dataset within the file, then open it to
create and attach a new index, to finally query data from it.
Note that some of the calls may be moved to the higher level API, in order to
directly read data that corresponds to the result of the index query.

{\lstsetc
\begin{lstlisting}
#define NTUPLES 256

int
main(int argc, char *argv[])
{
    float data[NTUPLES];
    hsize_t dims[1] = {NTUPLES};
    hid_t t file_id, dataspace_id, dataset_id;
    hid_t query_id, result_space_id;
    size_t result_npoints;
    float *result;
    int i;

    /* Initialize data. */
    for(i = 0; i < NTUPLES; i++) data[i] = (float) i;

    /* Create file. */
    file_id = H5Fcreate(file_name, H5F_ACC_TRUNC, H5P_DEFAULT, H5P_DEFAULT);

    /* Create the data space for the dataset. */
    dataspace_id = H5Screate_simple(rank, dims, NULL);

    /* Create dataset. */
    dataset_id = H5Dcreate(file_id, "Pressure", H5T_NATIVE_FLOAT, dataspace_id,
        H5P_DEFAULT, H5P_DEFAULT, H5P_DEFAULT);

    /* Write dataset. */
    H5Dwrite(dataset_id, H5T_NATIVE_FLOAT, H5S_ALL, H5S_ALL, H5P_DEFAULT, data);

    /* Close the dataset. */
    H5Dclose(dataset_id);

    /* Close dataspace. */
    H5Sclose(dataspace_id);

    /* Open dataset. */
    dataset_id = H5Dopen(file_id, "Pressure", H5P_DEFAULT);

    /* Create index using FastBit. */
    H5Xcreate(file_id, H5X_PLUGIN_FASTBIT, dataset_id, H5P_DEFAULT);

    /* Close the dataset. */
    H5Dclose(dataset_id);

    /* Create a simple query */
    query_id = H5Qcreate(H5Q_TYPE_DATA_ELEM, H5Q_MATCH_EQUAL, H5T_NATIVE_FLOAT,
        &query_value);

    /* Open dataset. */
    dataset_id = H5Dopen(file_id, "Pressure", H5P_DEFAULT);

    /* Use query to select elements in the dataset. */
    result_space_id = H5Dquery(dataset_id, query_id);

    /* Allocate space to read data. */ 
    result_npoints = (size_t) H5Sget_select_npoints(result_space_id);
    result = malloc(result_npoints * sizeof(float));

    /* Read data using result_space_id. */
    H5Dread(dataset_id, H5T_NATIVE_FLOAT, H5S_ALL, result_space_id,
        H5P_DEFAULT, result);

    /* Use result. */

    /* Free result. */
    free(result);

    /* Close the dataset. */
    H5Dclose(dataset_id);

    /* Close dataspace. */
    H5Sclose(result_space_id);

    /* Close query. */
    H5Qclose(query_id);

    /* Close the file. */
    H5Fclose(file_id);
}
\end{lstlisting}
}

\section{Conclusion}

\section*{Revision History}
\makerevisions

%% References
\bibliographystyle{ieeetr}
\bibliography{analysis_ext}

\end{document}
