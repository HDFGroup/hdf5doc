<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<HTML>

<HEAD>
  <TITLE>Scale and Offset Data Compression in HDF5</TITLE>
  <META http-equiv="content-type" content="text/html; charset=ISO-8859-1">
  <META name="author" content="Quincey Koziol">
</HEAD>

<BODY TEXT="#000000" BGCOLOR="#FFFFFF">

<STYLE TYPE="text/css">
OL.loweralpha { list-style-type: lower-alpha }
OL.upperroman { list-style-type: upper-roman }
</STYLE>
       
<CENTER><H1>Scale and Offset Data Compression in HDF5</H1></CENTER>
<CENTER><H3>Quincey Koziol<br>
            koziol@ncsa.uiuc.edu<BR>
            October 25, 2004
</H3></CENTER>

<OL class="upperroman">

<LI><H3><U>Document's Audience:</U></H3>
         
<UL>
    <LI>Current H5 library designers and knowledgable external developers.</li>
</UL>
         
<!--
<LI><H3><U>Background Reading:</U></H3>
         
<DL>
    <DT>The HDF5 reference manual sections for H5Tset_precision() and H5Tset_offset():
        <DD><A HREF="http://hdf.ncsa.uiuc.edu/HDF5/doc/RM_H5T.html#Datatype-SetPrecision"><CODE>H5Tset_precision</CODE></A>
        <DD><A HREF="http://hdf.ncsa.uiuc.edu/HDF5/doc/RM_H5T.html#Datatype-SetOffset"><CODE>H5Tset_offset</CODE></A>
</DL>
-->
         
<LI><H3><U>Introduction:</U></H3>
            
<DL>
    <DT><STRONG>What is this document about?</STRONG></DT>
        <DD>This document describes a new method of compressing integer and
            floating-point data in HDF5.
        </DD> <BR>

    <DT><STRONG>What is "scale and offset" compression?</STRONG></DT>
        <DD>Scale and offset compression is loosely defined as a method for
            performing a scale and/or offset operation on each data value, followed
            by truncating the resulting value to a lesser number of bits before
            storing it.
        </DD> <BR>

    <DT><STRONG>Will this compression method work on all HDF5 datatypes?</STRONG></DT>
        <DD>No, this compression method will only work properly on integer
            or floating-point data values.  I suppose a compound or array
            datatype consisting of just integers and/or floats would work, but
            making that work might be beyond what we should attempt initially.
        </DD> <BR>

</DL>

<LI><H3><U>Ideas & Problems:</U></H3>

<P>
I think that integer values are fairly easy to handle with a scale and offset
followed by a truncation and I've outlined some scenarios below.
<UL>
    <LI>
        <P>
        Integer values can be scanned by the library to determine the range
        of values to compress and the minimum # of bits to encode them can
        be determined algorithmically.  Consider the following array of values
        to compress:
        <BR> <BR>
        <table border=1>
        <tr>
            <td>4250 <td>4261 <td>4929
        <tr>
            <td>1021 <td>4656 <td>2712
        <tr>
            <td>3113 <td>3118 <td>2508
        </table>
        <BR>
        </P>

        <P>The minimum data value is 1021 and the maximum data value is 4929.
        Therefore the "span" of the values (<code>(max-min)+1</code>) is 3909
        and the minimum
        number of bits (<code>MinBits</code>) to store the values between the
        minimum and
        maximum values is: <code>ceiling(log<sub>2</sub>(span)) = 12 </code>.
        Setting the
        offset for the compression method to the minimum value, the values
        to pack are generated by this equation:
        <code>packed<sub>n</sub> = (unpacked<sub>n</sub> - offset) &amp;
        ((1 << MinBits) -1) </code>, where <code>n</code> indicates the
        n<sup>th</sup> value to operate on.
        Each packed value (of <code>MinBits</code> size) is then stored
        contiguously in a buffer without intervening bits between adjacent
        values.
        </P>

        <P>
        To unpack the values, each packed value of <code>MinBits</code> is
        zero extended to the original values' size in bits and then the
        <code>offset</code> is added to it.
        </P>
    </LI>

    <LI>
        <P>
        The previous example was simplified by not taking fill-values into
        account, which would not work well in practice due to fill-values
        frequently being the minimum or maximum value encodable for a given
        number of bits.  These values would throw the <code>MinBits</code>
        calculation
        off, possibly giving no compression savings at all.  If a fill-value is
        defined for a dataset, the fill-value should be ignored for the purposes of
        calculating the <code>span</code> of values, but the <code>MinBits</code>
        equation must be modified to be: <code>ceiling(log<sub>2</sub>(span+1))</code>.
        </P>
        
        <P>
        Additionally, the equations for computing the packed and unpacked
        values must be updated: <BR>
        <UL>
            <LI><code>packed<sub>n</sub> = (unpacked<sub>n</sub> == fill-value)
                    ? (( 1 << MinBits) - 1) : (unpacked<sub>n</sub> - offset) </code>
            <LI><code>unpacked<sub>n</sub> = (packed<sub>n</sub> == (( 1 << MinBits) - 1) )
                    ? fill-value : (packed<sub>n</sub> + offset) </code>
        </UL>
        </P>
    </LI>

    <LI>
        <P>
        In addition to allowing integer values to have the number of bits to
        store each value determined automatically by the library, it would also
        make sense to allow applications to explicitly control the number of
        bits used to store the data values.  Therefore, instead of determining
        <code>MinBits</code> algorithmically as above, the application supplies
        a <code>MinBits</code> value which is used to truncate the packed
        values, after the buffer's offset is subtracted.  Obviously, if an
        application supplied a <code>MinBits</code> which was less than that
        which would have been calculated for the automatic encoding method, the
        compression will be lossy.
        </P>

        <P>
        It's been suggested that different regions of a dataset could retain
        different numbers of bits, but I think this will be very complex to
        specify for users in the general case and I wouldn't recommend it, at
        least initially.
        </P>
    </LI>

    <LI>
        <P>
        Alternatively, we could give the application complete control over
        the scaling factor, the offset and the number of bits to retain, but
        that may be too inflexible over all the data values for the dataset.
        </P>
    </LI>
</UL>
</P>

<P>
It's straightforward to determine the appropriate number of bits necessary to
encode integer values, but floating-point values are quite a bit more difficult.
After tossing around various ideas, I think it's more practical to adopt the
GRiB data packing mechanism, outlined here: <A href="http://www-imk.fzk.de/asf/kasima/aktuelles/grib/datapack.html">
GRiB data packing method</A>.
</P>

<LI><H3><U>Discussion:</U></H3>

<P>
Automatic integer packing seems straightforward and shouldn't be terribly
difficult to implement, even with the quirks necessary for handling fill-values
properly.  Allowing users to determine the number of bits to retain shouldn't
add in any significant additional complexity.  I don't recommend having the
application specify the scale, offset and number of bits because those values
would be global to the entire dataset and would not be able to adjust to local
variations in the values for each chunk.
</P>

<P>
The GRiB data packing method looks complex, but many of our user communities are
familiar with it and would benefit from our implementing it properly.
</P>

<P>
After we've got the basic algorithms working for integer and floating-point
values, we might consider applying this form of compression to compound or
array datatypes as well.  This may be fairly complex, given than non-compressible
datatypes (like strings, etc) may be fields in a compound datatype as well and
the compression algorithm should not operate on them and just pass them along
unmodified.
</P>

</OL>

</BODY>
</HTML>

