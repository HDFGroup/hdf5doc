<!doctype HTML public "-//W3C//DTD HTML 4.0 Frameset//EN">
<html>
<head>
<title>Chapter 10: Additional Resources</title>

<!--(Meta)==========================================================-->


<!--(Links)=========================================================-->

<link href="ed_styles/NewUGelect.css" rel="stylesheet" type="text/css">

<!--( Begin styles definition )=====================================-->
<!--     Replaced with external stylesheet 'styles_NewUG.css'.      -->
<!--( End styles definition )=======================================-->

</head>

<body>

<!-- #BeginLibraryItem "/ed_libs/Copyright.lbi" -->
<!--
  * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * *
  * Copyright by The HDF Group.                                               *
  * Copyright by the Board of Trustees of the University of Illinois.         *
  * All rights reserved.                                                      *
  *                                                                           *
  * This file is part of HDF5.  The full HDF5 copyright notice, including     *
  * terms governing use, modification, and redistribution, is contained in    *
  * the files COPYING and Copyright.html.  COPYING can be found at the root   *
  * of the source code distribution tree; Copyright.html can be found at the  *
  * root level of an installed copy of the electronic HDF5 document set and   *
  * is linked from the top-level documents page.  It can also be found at     *
  * http://www.hdfgroup.org/HDF5/doc/Copyright.html.  If you do not have      *
  * access to either file, you may request a copy from help@hdfgroup.org.     *
  * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * *
 -->
<!-- #EndLibraryItem -->

<!-- HEADER LEFT "HDF5 User's Guide" -->
<!-- HEADER RIGHT "Additional Resources" -->

<div align="center">
<a name="TOP">
<h2>Chapter 10<br /><font size="7">Additional Resources</font></h2>
</a>
</div>

<!-- FOR USE WITH ELECTRONIC VERSION --------------------------------->

<center>
<table border=0 width=80%>

    <tr>
      <td valign=top colspan=3>
      <p>These documents supplement the <cite>HDF5 User&rsquo;s Guide</cite>
      and provide additional detailed information for the use and tuning 
      of specific HDF5 features.</p></td>
      </tr>

    <tr>
      <td>&nbsp;&nbsp;&nbsp;</td>
      <td>&nbsp;&nbsp;&nbsp;</td>
      <td>&nbsp;&nbsp;&nbsp;</td>
      </tr>

    <tr>
      <td valign=top>
      <a href="http://www.hdfgroup.org/ftp/HDF5/examples/examples-by-api/">
      HDF5 Examples</a></td>
      <td>&nbsp;&nbsp;&nbsp;</td>
      <td valign=top>Code examples by API. 
        <!-- 11.2.10, keep code examples at the top of the list of links -->
        </td>
      </tr>

    <tr><td colspan=3>&nbsp;</td></tr>

    <tr>
      <td valign=top>
      <a href="../Advanced/Chunking/index.html">Chunking in HDF5</a></td>
      <td>&nbsp;&nbsp;&nbsp;</td>
      <td valign=top>Structuring the use of chunking and tuning it for 
      performance.</td>
      </tr>

    <tr><td colspan=3>&nbsp;</td></tr>

    <tr>
      <td valign=top>
      <a href="../Advanced/DirectChunkWrite/UsingDirectChunkWrite.pdf">
      Using the Direct Chunk Write Function</a></td>
      <td>&nbsp;&nbsp;&nbsp;</td>
      <td valign=top>Describes another way that chunks can be written 
      to datasets.</td>
      </tr>

    <tr><td colspan=3>&nbsp;</td></tr>

    <tr>
      <td valign=top>
      <a href="../Advanced/CommittedDatatypeCopying/CopyingCommittedDatatypesWithH5Ocopy.pdf">
      Copying Committed Datatypes with H5Ocopy</a></td>
      <td>&nbsp;&nbsp;&nbsp;</td>
      <td valign=top>
      Describes how to copy to another file a dataset that uses a committed 
      datatype or an object with an attribute that uses a committed datatype 
      so that the committed datatype in the destination file can be used by 
      multiple objects.</td>
      </tr>

    <tr><td colspan=3>&nbsp;</td></tr>

    <tr>
      <td valign=top><a href="../Advanced/MetadataCache/index.html">
      Metadata Caching in HDF5</a></td>
      <td>&nbsp;&nbsp;&nbsp;</td>
      <td valign=top>Managing the HDF5 metadata cache and tuning it for 
      performance.</td>
      </tr>

    <tr><td colspan=3>&nbsp;</td></tr>

    <tr><td valign=top>
      <a href="../Advanced/DynamicallyLoadedFilters/HDF5DynamicallyLoadedFilters.pdf">
      HDF5 Dynamically Loaded Filters</a>
      <td>&nbsp;</td>
      <td valign=top>
      Describes how an HDF5 application can apply a non-native HDF5 
      filter during I/O operations at run-time. 
      </td>
      </tr>

    <tr><td colspan=3>&nbsp;</td></tr>

    <tr><td valign=top>
      <a href="../Advanced/FileImageOperations/HDF5FileImageOperations.pdf">
      HDF5 File Image Operations</a></td>
      <td>&nbsp;</td>
      <td valign=top>
      Describes how to work with HDF5 files in memory. Disk I/O is not 
      required when file images are opened, created, read from, or 
      written to.</td>
      </tr>

    <tr><td colspan=3>&nbsp;</td></tr>

    <tr><td valign=top><a href="../Advanced/UsingIdentifiers/index.html">
      Using Identifiers</a></td>
      <td>&nbsp;&nbsp;&nbsp;</td>
      <td valign=top>
      Describes how identifiers behave and how they should be treated.</td>
      </tr>

    <tr><td colspan=3>&nbsp;</td></tr>
    
    <tr><td valign=top><a href="../Advanced/UsingUnicode/index.html">
      Using UTF-8 Encoding in </a>
      <br />
      <a href="Advanced/UsingUnicode/index.html">HDF5 Applications</a>
      
      </td>
      <td>&nbsp;&nbsp;&nbsp;</td>
      <td valign=top>
      Describes the use of UTF-8 Unicode character encodings in HDF5 
      applications.</td>
      </tr>
      
    <tr><td colspan=3>&nbsp;</td></tr>
    
    <tr>
      <td valign=top><a href="../Glossary.html">
      HDF5 Glossary</a></td>
      <td>&nbsp;&nbsp;&nbsp;</td>
      <td valign=top>A glossary of terms.
      <!-- 11.2.10, keep Glossary at the bottom of the list of links --></td>
      </tr>

      <tr><td colspan=3>&nbsp;</td></tr>
      




</table>
</center>









<!-- FOR USE WITH ELECTRONIC VERSION --------------------------------->

<!-- PRINT VERSION INCLUDES THE TEXT TO THE END OF THE FILE -->
<!-- FOR USE WITH PRINT VERSION --------------------------------------



<p> This chapter provides supplemental material for the 
    <cite>HDF5 User&rsquo;s Guide</cite>. </p>

<p> To see code examples by API, go to the <cite>HDF5 Examples</cite> 
    page at this address: 
    <pre>    http://www.hdfgroup.org/ftp/HDF5/examples/examples-by-api/
    </pre></p>
<p> For more information on how to manage the metadata cache and how 
    to configure it for better performance, go to the <cite>Metadata
    Caching in HDF5</cite> page at this address:
    <pre>
    http://www.hdfgroup.org/hdf5/doc/Advanced/MetadataCache/index.html
    </pre></p>
<p> A number of functions are macros. For more information on how
    to use the macros, see the <cite>API Compatibility Macros in HDF5</cite>
    page at this address:
    <pre>
    http://www.hdfgroup.org/HDF5/doc/RM/APICompatMacros.html
    </pre>
    
<p>The following sections are included in this chapter:</p>
<ul>
    <li><i>Using Identifiers</i> - describes how identifiers behave
    and how they should be treated
    <li><i>Chunking in HDF5</i> - describes chunking storage and 
    how it can be used to improve performance
    <li><i>HDF5 Glossary and Terms</i></li>
</ul>
    

<br />
<!-- NEW PAGE -->
<!-- PRINT VERSION CONTINUED --
<h2>10.1. Using Identifiers</h2>
<p> The purpose of this section is to describe how identifiers behave 
    and how they should be treated by application programs.</p>

<p> When an application program uses the HDF5 library to create or 
    open an item, a unique identifier is returned. The items that return 
    a unique identifier when they are created or opened include the 
    following: dataset, group, datatype, dataspace, file, attribute, 
    property list, referenced object, error stack, and error message.</p>

<p> An application may open one of the items listed above more than 
    once at the same time. For example, an application might open a group 
    twice, receiving two identifiers. Information from one dataset in the 
    group could be handled through one identifier, and the information 
    from another dataset in the group is handled by a different identifier.</p>

<p> An application program should track every identifier it receives 
    as a result of creating or opening one of the items listed above. In 
    order for an application to close properly, it must release every 
    identifier it has opened. If an application opened a group twice for 
    example, it would need to issue two <code>H5Gclose</code> commands, 
    one for each identifier. Not releasing identifiers causes resource 
    leaks. Until an identifier is released, the item associated with 
    the identifier is still open.</p>

<p> The library considers a file open until all of the identifiers 
    associated with the file and with the file’s various items have been 
    released. The identifiers associated with these open items must be 
    released separately. This means that an application can close a file 
    and still work with one or more portions of the file. Suppose an 
    application opened a file, a group within the file, and two datasets 
    within the group. If the application closed the file with 
    <code>H5Fclose</code>, then the file would be considered closed 
    to the application, but the group and two datasets would still 
    be open.</p>

<p> There are several exceptions to the above file closing rule. One 
    is when the <code>H5close</code> function is used instead of 
    <code>H5Fclose</code>. <code>H5close</code> causes a general 
    shutdown of the library: all data is written to disk, all 
    identifiers are closed, and all memory used by the library is 
    cleaned up. Another exception occurs on parallel processing systems. 
    Suppose on a parallel system an application has opened a file, a 
    group in the file, and two datasets in the group. If the application 
    uses the <code>H5Fclose</code> function to close the file, the call 
    will fail with an error. The open group and datasets must be closed 
    before the file can be closed. A third exception is when the file 
    access property list includes the property 
    <code>H5F_CLOSE_STRONG</code>. This property causes the closing 
    of all of the file’s open items when the file is closed with 
    <code>H5Fclose</code>.</p>

<p> For more information about <code>H5close</code>, 
    <code>H5Fclose</code>, and <code>H5Pset_fclose_degree</code>, 
    see the <a href="../RM/RM_H5Front.html">
    <cite>HDF5 Reference Manual</cite></a></p>

<!--
---------1---------2---------3---------4---------5---------6---------7---------8
-->
<!-- PRINT VERSION CONTINUED --
<h3>Functions that Return Identifiers</h3>

<p>Some of the functions that return identifiers are listed below.</p>

<ul>
<li><code>H5Acreate</code></li>
<li><code>H5Acreate_by_name</code></li>
<li><code>H5Aget_type</code></li>
<li><code>H5Aopen</code></li>
<li><code>H5Aopen_by_idx</code></li>
<li><code>H5Aopen_by_name</code></li>
<li><code>H5Dcreate</code></li>
<li><code>H5Dcreate_anon</code></li>
<li><code>H5Dget_access_plist</code></li>
<li><code>H5Dget_create_plist</code></li>
<li><code>H5Dget_space</code></li>
<li><code>H5Dget_type</code></li>
<li><code>H5Dopen</code></li>
<li><code>H5Ecreate_msg</code></li>
<li><code>H5Ecreate_stack</code></li>
<li><code>H5Fcreate</code></li>
<li><code>H5Fopen</code></li>
<li><code>H5Freopen</code></li>
<li><code>H5Gcreate</code></li>
<li><code>H5Gcreate_anon</code></li>
<li><code>H5Gopen</code></li>
<li><code>H5Oopen</code></li>
<li><code>H5Oopen_by_addr</code></li>
<li><code>H5Oopen_by_idx</code></li>
<li><code>H5Pcreate</code></li>
<li><code>H5Rdereference</code></li>
<li><code>H5Rget_region</code></li>
<li><code>H5Screate</code></li>
<li><code>H5Screate_simple</code></li>
<li><code>H5Tcopy</code></li>
<li><code>H5Tcreate</code></li>
<li><code>H5Tdecode</code></li>
<li><code>H5Tget_member_type</code></li>
<li><code>H5Tget_super</code></li>
<li><code>H5Topen</code></li>
</ul>

<br />



<!-- NEW PAGE -->
<!-- PRINT VERSION CONTINUED --
<h2>10.2. Chunking in HDF5</h2>

<p> Datasets in HDF5 not only provide a convenient, structured, and 
    self-describing way to store data, but are also designed to do so with 
    good performance. In order to maximize performance, the HDF5 library 
    provides ways to specify how the data is stored on disk, 
    how it is accessed, and how it should be held in memory.</p>
<!--
---------1---------2---------3---------4---------5---------6---------7---------8
-->
<!-- PRINT VERSION CONTINUED --
<h3>10.2.1. What are Chunks?</h3>

<p> Datasets in HDF5 can represent arrays with any number of dimensions 
    (up to 32). However, in the file this dataset must be stored as part 
    of the 1-dimensional stream of data that is the low-level file. 
    The way in which the multidimensional dataset is mapped to the 
    serial file is called the layout. The most obvious way to accomplish 
    this is to simply flatten the dataset in a way similar to how arrays 
    are stored in memory, serializing the entire dataset into a 
    monolithic block on disk, which maps directly to a memory buffer 
    the size of the dataset. This is called a contiguous layout.</p>
    
<p> An alternative to the contiguous layout is the chunked layout. 
    Whereas contiguous datasets are stored in a single block in the file, 
    chunked datasets are split into multiple <em>chunks</em> which are 
    all stored separately in the file. The chunks can be stored in any 
    order and any position within the HDF5 file. Chunks can then be read 
    and written individually, improving performance when operating on 
    a subset of the dataset. </p>
    
<p> The API functions used to read and write chunked datasets are 
    exactly the same functions used to read and write contiguous 
    datasets. The only difference is a single call to set up the layout 
    on a property list before the dataset is created. In this way, a 
    program can switch between using chunked and contiguous datasets 
    by simply altering that call. Example 1, below, creates a dataset 
    with a size of 12x12 and a chunk size of 4x4. The example could be 
    change to create a contiguous dataset instead by simply commenting 
    out the call to <code>H5Pset_chunk</code>.</p>

<!-- NEW PAGE -->
<!-- PRINT VERSION CONTINUED --

<table width="600" cellspacing="0" align="center">
    <tr valign="top"> 
        <td align="left">
        <hr color="green" size="3"/>
        <pre>
#include <hdf5.h>
int main(void) {
    hid_t   file_id, dset_id, space_id, dcpl_id;
    hsize_t chunk_dims[2] = {4, 4};
    hsize_t dset_dims[2] = {12, 12};
    int     buffer[12][12];

    /* Create the file */
    file_id = H5Fcreate(file.h5, H5F_ACC_TRUNC, H5P_DEFAULT, H5P_DEFAULT);

    /* Create a dataset creation property list and set it to use chunking
    */
    dcpl_id = H5Pcreate(H5P_DATASET_CREATE);
    H5Pset_chunk(dcpl_id, 2, chunk_dims);

    /* Create the dataspace and the chunked dataset */
    space_id = H5Screate_simple(2, dset_dims, NULL);
    dset_id = H5Dcreate(file, dataset, H5T_NATIVE_INT, space_id, dcpl_id, H5P_DEFAULT);

    /* Write to the dataset */
    buffer = <initialize buffer>
    H5Dwrite(dset_id, H5T_NATIVE_INT, H5S_ALL, H5S_ALL, H5P_DEFAULT, buffer);

    /* Close */
    H5Dclose(dset_id);
    H5Sclose(space_id);
    H5Pclose(dcpl_id);
    H5Fclose(file_id);
    return 0;
}
    </pre></td>
  </tr>
    <tr><td><hr color="green" size="1" /></td></tr>
    <tr valign="top">
        <td align="left">
        <b>Example 1. Creating a chunked dataset</b>
        <hr color="green" size="3"/></td>
        </tr>
</table>
<br />




<p> The chunks of a chunked dataset are split along logical boundaries 
    in the dataset's representation as an array, not along boundaries 
    in the serialized form. Suppose a dataset has a chunk size of 2x2. 
    In this case, the first chunk would go from (0,0) to (2,2), the 
    second from (0,2) to (2,4), and so on. By selecting the chunk size 
    carefully, it is possible to fine tune I/O to maximize performance 
    for any access pattern. Chunking is also required to use advanced 
    features such as compression and dataset resizing.</p>


<!-- NEW Page -->
<!-- PRINT VERSION CONTINUED --

<table width="600" cellspacing="0" align="center">
    <tr valign="top"> 
        <td align="center">
        <hr color="green" size="3"/>
        <br />
        <img src="Images/ChunkingFig001.png">
        <br /><br />
        </td></tr>
    <tr><td><hr color="green" size="1" /></td></tr>
    <tr valign="top">
        <td align="left" >
        <b>Figure 1. Contiguous dataset</b>
        <hr color="green" size="3"/></td></tr>
    </table>
<br />



<!-- NEW Page -->
<!-- PRINT VERSION CONTINUED --

<table width="600" cellspacing="0" align="center">
    <tr valign="top"> 
        <td align="center">
        <hr color="green" size="3"/>
        <br />
        <img src="Images/ChunkingFig002.png">
        <br /><br />
        </td></tr>
    <tr><td><hr color="green" size="1" /></td></tr>
    <tr valign="top">
        <td align="left" >
        <b>Figure 2. Chunked dataset</b>
        <hr color="green" size="3"/></td></tr>
    </table>
<br />




<!-- NEW PAGE -->
<!-- PRINT VERSION CONTINUED --
<h3>10.2.2. Data Storage Order</h3>

<p> To understand the effects of chunking on I/O performance it is 
    necessary to understand the order in which data is actually stored 
    on disk.  When using the C interface, data elements are stored in 
    "row-major" order, meaning that, for a 2-dimensional dataset, rows 
    of data are stored in-order on the disk. This is equivalent to the 
    storage order of C arrays in memory.</p>
    
<p> Suppose we have a 10x10 contiguous dataset B. The first element 
    stored on disk is B[0][0], the second B[0][1], the eleventh B[1][0], 
    and so on. If we want to read the elements from B[2][3] to B[2][7], 
    we have to read the elements in the 24th, 25th, 26th, 27th, and 28th 
    positions. Since all of these positions are contiguous, or next to 
    each other, this can be done in a single read operation: read 5 
    elements starting at the 24th position. This operation is illustrated 
    in figure 3: the pink cells represent elements to be read and the 
    solid line represents a read operation. Now suppose we want to 
    read the elements in the column from B[3][2] to B[7][2]. In this case 
    we must read the elements in the 33rd, 43rd, 53rd, 63rd, and 73rd 
    positions. Since these positions are not contiguous, this must be 
    done in 5 separate read operations. This operation is illustrated 
    in figure 4: the solid lines again represent read operations, and 
    the dotted lines represent seek operations. An alternative would 
    be to perform a single large read operation , in this case 41 
    elements starting at the 33rd position. This is called a 
    <em>sieve buffer</em> and is supported by HDF5 for contiguous 
    datasets, but not for chunked datasets. By setting the chunk sizes 
    correctly, it is possible to greatly exceed the performance of the 
    sieve buffer scheme.</p>


<table width="600" cellspacing="0" align="center">
    <tr valign="top"> 
        <td align="center">
        <hr color="green" size="3"/>
        <br />
        <img src="Images/ChunkingFig003.png">
        <br /><br />
        </td>
        </tr>
    <tr><td><hr color="green" size="1" /></td></tr>
    <tr valign="top">
        <td align="left" >
        <b>Figure 3. Reading part of a row from a contiguous dataset</b>
        <hr color="green" size="3"/></td>
        </tr>
  </table>
<br />
<br />


<table width="600" cellspacing="0" align="center">
    <tr valign="top"> 
        <td align="center">
        <hr color="green" size="3"/>
        <br />
        <img src="Images/ChunkingFig004.png" alt="Illustration of a 
        partial column of a contiguous dataset">
        <br /><br />
        </td>
        </tr>
    <tr><td><hr color="green" size="1" /></td></tr>
    <tr valign="top">
        <td align="left" >
        <b>Figure 4. Reading part of a column from a contiguous dataset</b>
        <hr color="green" size="3"/></td>
        </tr>
</table>
<br />



<p> Likewise, in higher dimensions, the last dimension specified is the 
    fastest changing on disk. So if we have a four dimensional dataset A, 
    then the first element on disk would be A[0][0][0][0], the 
    second A[0][0][0][1], the third A[0][0][0][2], and so on.</p>

<h3>10.2.3. Chunking and Partial I/O</h3>

<p> The issues outlined above regarding data storage order help to 
    illustrate one of the major benefits of dataset chunking, its 
    ability to improve the performance of partial I/O. Partial I/O 
    is an I/O operation (read or write) which operates on only one 
    part of the dataset. To maximize the performance of partial I/O, 
    the data elements selected for I/O must be contiguous on disk. 
    As we saw above, with a contiguous dataset, this means that the 
    selection must always equal the extent in all but the slowest 
    changing dimension, unless the selection in the slowest changing 
    dimension is a single element. With a 2-d dataset in C, this 
    means that the selection must be as wide as the entire dataset 
    unless only a single row is selected. With a 3-d dataset, this 
    means that the selection must be as wide and as deep as the 
    entire dataset, unless only a single row is selected, in which 
    case it must still be as deep as the entire dataset, unless 
    only a single column is also selected.</p>
  
<p> Chunking allows the user to modify the conditions for maximum 
    performance by changing the regions in the dataset which are 
    contiguous. For example, reading a 20x20 selection in a contiguous 
    dataset with a width greater than 20 would require 20 separate 
    and non-contiguous read operations. If the same operation were 
    performed on a dataset that was created with a chunk size of 
    20x20, the operation would require only a single read operation. 
    In general, if your selections are always the same size (or 
    multiples of the same size), and start at multiples of that 
    size, then the chunk size should be set to the selection size, 
    or an integer divisor of it. This recommendation is subject to 
    the guidelines in the <em>pitfalls</em> section; specifically, 
    it should not be too small or too large.</p>
  
<p> Using this strategy, we can greatly improve the performance of the 
    operation shown in figure 4. If we create the dataset with a chunk 
    size of 10x1, each column of the dataset will be stored separately 
    and contiguously. The read of a partial column can then be done is 
    a single operation. This is illustrated in figure 5, and the code 
    to implement a similar operation is shown in example 2. For 
    simplicity, example 2 implements writing to this dataset instead 
    of reading from it.</p>


<table width="600" cellspacing="0" align="center">
    <tr valign="top"> 
        <td align="center">
        <hr color="green" size="3"/>
        <br />
        <img src="Images/ChunkingFig005.png">
        <br /><br />
        </td></tr>
    <tr><td><hr color="green" size="1" /></td></tr>
    <tr valign="top">
        <td align="left" >
        <b>Figure 5. Reading part of a column from a chunked dataset</b>
        <hr color="green" size="3"/></td></tr>
</table>
<br />
<br />



    
<!-- NEW PAGE -->
<!-- PRINT VERSION CONTINUED --
    
<table width="600" cellspacing="0" align="center">
    <tr valign="top"> 
        <td align="left">
        <hr color="green" size="3"/>
        <pre>
#include <hdf5.h>
int main(void) {
    hid_t   file_id, dset_id, fspace_id, mspace_id, dcpl_id;
    hsize_t chunk_dims[2] = {10, 1};
    hsize_t dset_dims[2] = {10, 10};
    hsize_t mem_dims[1] = {5};
    hsize_t start[2] = {3, 2};
    hsize_t count[2] = {5, 1};
    int     buffer[5];

    /* Create the file */
    file_id = H5Fcreate(<em>file.h5</em>, H5F_ACC_TRUNC, H5P_DEFAULT, H5P_DEFAULT);

    /* Create a dataset creation property list and set it to use chunking
    * with a chunk size of 10x1 */
    dcpl_id = H5Pcreate(H5P_DATASET_CREATE);
    H5Pset_chunk(dcpl_id, 2, chunk_dims);

    /* Create the dataspace and the chunked dataset */
    space_id = H5Screate_simple(2, dset_dims, NULL);
    dset_id = H5Dcreate(file, <em>dataset</em>, H5T_NATIVE_INT, space_id, dcpl_id, 
H5P_DEFAULT);

    /* Select the elements from 3, 2 to 7, 2 */
    H5Sselect_hyperslab(fspace_id, H5S_SELECT_SET, start, NULL, count, NULL);

    /* Create the memory dataspace */
    mspace_id = H5Screate_simple(1, mem_dims, NULL);

    /* Write to the dataset */
    buffer = <initialize buffer>
    H5Dwrite(dset_id, H5T_NATIVE_INT, mspace_id, fpsace_id, H5P_DEFAULT, buffer);

    /* Close */
    H5Dclose(dset_id);
    H5Sclose(fspace_id);
    H5Sclose(mspace_id);
    H5Pclose(dcpl_id);
    H5Fclose(file_id);
    return 0;
}</pre></td>
        </tr>
    <tr><td><hr color="green" size="1" /></td></tr>
    <tr valign="top">
        <td align="left">
        <b>Example 2. Writing part of a column to a chunked dataset</b>
        <hr color="green" size="3"/></td>
        </tr>
</table>
<br />
    



<h3>10.2.4. Chunk Caching</h3>

<p> Another major feature of the dataset chunking scheme is the chunk 
    cache.  As it sounds, this is a cache of the chunks in the dataset. 
    This cache can greatly improve performance whenever the same chunks 
    are read from or written to multiple times, by preventing the 
    library from having to read from and write to disk multiple times. 
    However, the current implementation of the chunk cache does not adjust 
    its parameters automatically, and therefore the parameters must be 
    adjusted manually to achieve optimal performance. In some rare 
    cases it may be best to completely disable the chunk caching scheme. 
    Each open dataset has its own chunk cache, which is separate from 
    the caches for all other open datasets.</p>
  
<p> When a selection is read from a chunked dataset, the chunks containing 
    the selection are first read into the cache, and then the selected 
    parts of those chunks are copied into the user's buffer. The cached 
    chunks stay in the cache until they are evicted, which typically 
    occurs because more space is needed in the cache for new chunks, 
    but they can also be evicted if hash values collide (more on this 
    later). Once the chunk is evicted it is written to disk if necessary 
    and freed from memory.</p>
  
<p> This process is illustrated in figures 6 and 7. In figure 6, the 
    application requests a row of values, and the library responds by 
    bringing the chunks containing that row into cache, and retrieving 
    the values from cache. In figure 7, the application requests a 
    different row that is covered by the same chunks, and the library 
    retrieves the values directly from cache without touching the disk.</p>

<table width="600" cellspacing="0" align="center">
    <tr valign="top"> 
        <td align="center">
        <hr color="green" size="3"/>
        <br />
        <img src="Images/ChunkingFig006.png">
        <br /><br />
        </td></tr>
    <tr><td><hr color="green" size="1" /></td></tr>
    <tr valign="top">
        <td align="left" >
        <b>Figure 6. Reading a row from a chunked dataset with the 
        chunk cache enabled</b>
        <hr color="green" size="3"/></td></tr>
    </table>
<br />
<br />


<table width="600" cellspacing="0" align="center">
    <tr valign="top"> 
        <td align="center">
        <hr color="green" size="3"/>
        <br />
        <img src="Images/ChunkingFig007.png" alt="Illustration of chunk 
        caching and a row of a chunked dataset with the chunks already 
        in the cache">
        <br /><br />
        </td></tr>
    <tr><td><hr color="green" size="1" /></td></tr>
    <tr valign="top">
        <td align="left" >
        <b>Figure 7. Reading a row from a chunked dataset with the 
        chunks already cached</b>
        <hr color="green" size="3"/></td></tr>
</table>
<br />
<br />




  
<p> In order to allow the chunks to be looked up quickly in cache, each 
    chunk is assigned a unique hash value that is used to look up the 
    chunk. The cache contains a simple array of pointers to chunks, 
    which is called a hash table. A chunk's hash value is simply the 
    index into the hash table of the pointer to that chunk. While the 
    pointer at this location might instead point to a different chunk 
    or to nothing at all, no other locations in the hash table can 
    contain a pointer to the chunk in question. Therefore, the library 
    only has to check this one location in the hash table to tell if a 
    chunk is in cache or not. This also means that if two or more 
    chunks share the same hash value, then only one of those chunks can 
    be in the cache at the same time. When a chunk is brought into 
    cache and another chunk with the same hash value is already in 
    cache, the second chunk must be evicted first. Therefore it is 
    very important to make sure that the size of the hash table, also 
    called the nslots parameter in <code>H5Pset_cache</code> and 
    <code>H5Pset_chunk_cache</code>, is large enough to minimize the 
    number of hash value collisions.</p>
  
<p> To determine the hash value for a chunk, the chunk is first assigned 
    a unique index that is the linear index into a hypothetical array of 
    the chunks. That is, the upper-left chunk has an index of 0, the one 
    to the right of that has an index of 1, and so on. This index is 
    then divided by the size of the hash table, nslots, and the 
    remainder, or modulus, is the hash value. Because this scheme can 
    result in regularly spaced indices being used frequently, it is 
    important that nslots be a prime number to minimize the chance 
    of collisions. In general, nslots should probably be set to a 
    number approximately 100 times the number of chunks that can fit 
    in nbytes bytes, unless memory is extremely limited. There is of 
    course no advantage in setting nslots to a number larger than 
    the total number of chunks in the dataset.</p>
  
<p> The w0 parameter affects how the library decides which chunk to 
    evict when it needs room in the cache. If w0 is set to 0, then 
    the library will always evict the least recently used chunk in 
    cache. If w0 is set to 1, the library will always evict the least 
    recently used chunk which has been fully read or written, and if 
    none have been fully read or written, it will evict the least 
    recently used chunk. If w0 is between 0 and 1, the behaviour will 
    be a blend of the two. Therefore, if the application will access 
    the same data more than once, w0 should be set closer to 0, and 
    if the application does not, w0 should be set closer to 1.</p>
  
<p> It is important to remember that chunk caching will only give a 
    benefit when reading or writing the same chunk more than once. 
    If, for example, an application is reading an entire dataset, 
    with only whole chunks selected for each operation, then chunk 
    caching will not help performance, and it may be preferable to 
    completely disable the chunk cache in order to save memory. It 
    may also be advantageous to disable the chunk cache when writing 
    small amounts to many different chunks, if memory is not large 
    enough to hold all those chunks in cache at once.</p>

<h3>10.2.5. I/O Filters and Compression</h3>

<p> Dataset chunking also enables the use of I/O filters, including 
    compression. The filters are applied to each chunk individually, 
    and the entire chunk is processed at once. The filter must be 
    applied every time the chunk is loaded into cache, and every 
    time the chunk is flushed to disk. These facts all make choosing 
    the proper settings for the chunk cache and chunk size even 
    more critical for the performance of filtered datasets.</p>
  
<p> Because the entire chunk must be filtered every time disk I/O 
    occurs, it is no longer a viable option to disable the chunk 
    cache when writing small amounts of data to many different chunks. 
    To achieve acceptable performance, it is critical to minimize the 
    chance that a chunk will be flushed from cache before it is 
    completely read or written. This can be done by increasing the 
    size of the chunk cache, adjusting the size of the chunks, or 
    adjusting I/O patterns.</p>

<h3>10.2.6. Pitfalls</h3>

<p> Inappropriate chunk size and cache settings can dramatically reduce 
    performance. There are a number of ways this can happen. Some of 
    the more common issues include:

<ul>
    <li>Chunks are too small</li>
      <p> There is a certain amount of overhead associated with finding 
          chunks. When chunks are made smaller, there are more of them 
          in the dataset. When performing I/O on a dataset, if there 
          are many chunks in the selection, it will take extra time 
          to look up each chunk. In addition, since the chunks are 
          stored independently, more chunks results in more I/O 
          operations, further compounding the issue. The extra metadata 
          needed to locate the chunks also causes the file size to 
          increase as chunks are made smaller. Making chunks larger 
          results in fewer chunk lookups, smaller file size, and 
          fewer I/O operations in most cases.</p>
    <li>Chunks are too large</li>
      <p> It may be tempting to simply set the chunk size to be the 
          same as the dataset size in order to enable compression on 
          a <em>contiguous</em> dataset. However, this can have 
          unintended consequences. Because the entire chunk must be 
          read from disk and decompressed before performing any 
          operations, this will impose a great performance penalty 
          when operating on a small subset of the dataset if the cache 
          is not large enough to hold the one-chunk dataset. In 
          addition, if the dataset is large enough, since the entire 
          chunk must be held in memory while compressing and decompressing, 
          the operation could cause the operating system to page memory 
          to disk, slowing down the entire system.</p>
    <li>Cache is not big enough</li>
      <p> Similarly, if the chunk cache is not set to a large enough 
          size for the chunk size and access pattern, poor performance 
          will result. In general, the chunk cache should be large 
          enough to fit all of the chunks that contain part of a 
          hyperslab selection used to read or write. When the chunk 
          cache is not large enough, all of the chunks in the selection 
          will be read into cache and then written to disk (if writing) 
          and evicted. If the application then revisits the same 
          chunks, they will have to be read and possibly written 
          again, whereas if the cache were large enough they would 
          only have to be read (and possibly written) once. However, 
          if selections for I/O always coincide with chunk boundaries, 
          this does not matter as much, as there is no wasted I/O and 
          the application is unlikely to revisit the same chunks soon 
          after.</p>
      <p> If the total size of the chunks involved in a selection is too 
          big to practically fit into memory, and neither the chunk nor 
          the selection can be resized or reshaped, it may be better to 
          disable the chunk cache. Whether this is better depends on the 
          storage order of the selected elements. It will also make 
          little difference if the dataset is filtered, as entire chunks 
          must be brought into memory anyways in that case. When the 
          chunk cache is disabled and there are no filters, all I/O is 
          done directly to and from the disk. If the selection is mostly 
          along the fastest changing dimension (i.e. rows), then the 
          data will be more contiguous on disk, and direct I/O will be 
          more efficient than reading entire chunks, and hence the 
          cache should be disabled. If however the selection is mostly 
          along the slowest changing dimension (columns), then the 
          data will not be contiguous on disk, and direct I/O will 
          involve a large number of small operations, and it will 
          probably be more efficient to just operate on the entire 
          chunk, therefore the cache should be set large enough to 
          hold at least 1 chunk. To disable the chunk cache, either 
          nbytes or nslots should be set to 0.</p>
    <li>Improper hash table size</li>
      <p> Because only one chunk can be present in each slot of the hash 
          table, it is possible for an improperly set hash table size 
          (nslots) to severely impact performance. For example, if 
          there are 100 columns of chunks in a dataset, and the hash 
          table size is set to 100, then all the chunks in each row 
          will have the same hash value. Attempting to access a row 
          of elements will result in each chunk being brought into 
          cache and then evicted to allow the next one to occupy its 
          slot in the hash table, even if the chunk cache is large 
          enough, in terms of nbytes, to hold all of them. Similar 
          situations can arise when nslots is a factor or multiple of 
          the number of rows of chunks, or equivalent situations in 
          higher dimensions.</p>
      <p> Luckily, because each slot in the hash table only occupies 
          the size of the pointer for the system, usually 4 or 8 
          bytes, there is little reason to keep nslots small. Again, 
          a general rule is that nslots should be set to a prime 
          number at least 100 times the number of chunks that can 
          fit in nbytes, or simply set to the number of chunks in 
          the dataset.</p>
    </ul>

<h3>10.2.7. For More Information</h3>

<!--
---------1---------2---------3---------4---------5---------6---------7---------8
-->
<!-- the pdf mentioned in the paragraph below is not available to print 
readers (ie, it's not on the web site)
<p> The slide set &ldquo;
<a href="../_topic/Chunking/Chunking_Tutorial_EOS13_2009.pdf">HDF5 
    Advanced Topics: Chunking in HDF5</a>&rdquo; (PDF), a tutorial from 
    HDF and HDF-EOS Workshop XIII (2009) provides additional HDF5 chunking 
    use cases and examples.</p>
-->

<!-- FOR USE WITH ELECTRONIC VERSION ---------------------------------
<!-- 11.18.10, the paragraph below is labeled for the electronic version,
but it doesn't seem to work with the electronic page. I'm keeping this 
paragraph commented out. --
<p> The page &ldquo;<a href=
    "http://www.hdfgroup.org/ftp/HDF5/examples/examples-by-api/api18-c.html">
    HDF5 Examples by API</a>&rdquo; lists many code examples that are 
    regularly tested with the HDF5 Library.  Several illustrate the 
    use of chunking in HDF5, particularly &ldquo;Read/Write Chunked 
    Dataset&rdquo; and any examples demonstrating filters. </p>
<!-- FOR USE WITH ELECTRONIC VERSION --------------------------------->

<!-- FOR USE WITH PRINT VERSION -------------------------------------->
<!-- PRINT VERSION CONTINUED --
<p> The &ldquo;HDF5 Examples by API&rdquo; page, 
    <code>
    http:/www.hdfgroup.org/ftp/HDF5/examples/examples-by-api/api18-c.html,
    </code> 
    lists many code examples that are regularly tested with the HDF5 
    Library. Several illustrate the use of chunking in HDF5, particularly 
    &ldquo;Read/Write Chunked Dataset&rdquo; and any examples 
    demonstrating filters. </p>



<!-- FOR USE WITH PRINT VERSION --------------------------------------



<!-- FOR USE WITH ELECTRONIC VERSION --------------------------------->
<!-- 11.19.10, section 10.2.8. is labeled for the electronic version, 
but doesn't seem to be appropriate for the page. I'm leaving it commented 
out. --
<!-- 
<h3>10.2.8. Directions for Future Development</h3>

  As seen above, the HDF5 chunk cache currently requires careful control of the 
  parameters in order to achieve optimal performance. In the future, we plan to 
  improve the chunk cache to be more foolproof in many ways, and deliver acceptable 
  performance in most cases even when no thought is given to the chunking parameters.
  <p>
  One way to make the chunk cache more user-friendly is to automatically resize the 
  chunk cache as needed for each operation. The cache should be able to detect when 
  the cache should be skipped or when it needs to be enlarged based on the pattern of 
  I/O operations. At a minimum, it should be able to detect when the cache would 
  severely hurt performance for a single operation and disable the cache for that 
  operation. This would of course be optional.
  <p>
  Another way is to allow chaining of entries in the hash table. This would make the 
  hash table size much less of an issue, as chunks could shared the same hash value 
  by making a linked list.
  <p>
  Finally, it may even be desirable to set some reasonable default chunk size based 
  on the dataset size and possibly some other information on the intended access 
  pattern. This would probably be a high-level routine.
  <p>
  Other features planned for chunking include new index methods (besides b-trees), 
  disabling filters for chunks that are partially over the edge of a dataset, only 
  storing the used portions of these edge chunks, and allowing multiple reader 
  processes to read the same dataset as a single writer process writes to it.

<div align="right">
<table>
    <tr><td align="top" valign="left">
        <font size="6" color="AAAAAA">DRAFT&nbsp;&nbsp;&nbsp;&nbsp;</font>
    </td><td align="top" valign="left">
        <i>Chunking in HDF5</i> is under active development. Please 
        send comments, suggestions, and bug reports to 
        fbaker-at-hdfgroup.org.
    </td></tr>
</table>
</div>
-->

<!-- FOR USE WITH ELECTRONIC VERSION --------------------------------->
<!-- PRINT VERSION CONTINUED --
<br />
<!-- NEW PAGE -->
<!-- PRINT VERSION CONTINUED --
<h2>10.3. HDF5 Glossary and Terms</h2>


<dl>

<dt><strong><a name="Glossary-AtomicDType">atomic datatype</a></strong></dt>
    <dd>A datatype which cannot be decomposed into smaller units at the 
       API level. </dd>
<br />

<dt><a name="Glossary-Attribute"><b>attribute</b></a></dt>
<dd>A small dataset that can be used to describe the nature and/or 
    the intended usage of the object it is attached to.</dd>
<br />

<!--
<dt><strong><a name="Glossary-Basic">basic datatypes</a></strong></dt>
    <ul>
        <li>char     - 8-bit character (only for ASCII information)</li>
        <li>int8     - 8-bit signed integer</li>
        <li>uint8    - 8-bit unsigned integer</li>
        <li>int16    - 16-bit signed integer</li>
        <li>uint16   - 16-bit unsigned integer</li>
        <li>int32    - 32-bit signed integer</li>
        <li>uint32   - 32-bit unsigned integer</li>
        <li>intn     - "native" signed integer</li>
        <li>uintn    - "native" unsigned integer</li>
        <li>int64    - 64-bit signed integer (new)</li>
        <li>uint64   - 64-bit unsigned integer (new)</li>
        <li>float32  - 32-bit IEEE float</li>
        <li>float64  - 64-bit IEEE float</li>
    </ul>
<br />
-->
<!-- PRINT VERSION CONTINUED --
<dt><strong><a name="Glossary-LayoutChunked">chunked layout</a></strong></dt>
<dd>The storage layout of a chunked dataset.</dd>
<br />

<dt><strong><a name="Glossary-Chunking">chunking</a></strong></dt>
<dd>A storage layout where a dataset is partitioned into fixed-size 
    multi-dimensional chunks.  Chunking tends to improve performance
    and facilitates dataset extensibility.</dd>
<br />

<dt><strong><a name="Glossary-DTypeCommitted">committed datatype</a></strong></dt>
<dd>A datatype that is named and stored in a file so that it can be shared. 
    Committed datatypes can be shared. Committing is permanent; a datatype 
    cannot be changed after being committed. Committed datatypes used to be 
    called <a name="Glossary-DTypeNamed">named</a> datatypes.</dd>
<br />

<dt><strong><a name="Glossary-CompoundDType">compound datatype</a></strong></dt>
<dd>A collection of one or more atomic types or small arrays of such types.
    Similar to a struct in C or a common block in Fortran.</dd>
<br />

<!--
<dt><strong><a name="Glossary-ComplexDType">complex datatype</a></strong></dt>
<dd>A collection of one or more atomic types or small arrays of such types.
    <ul>
        <li>hid_t   - 32-bit unsigned integer used as ID for memory objects</li>
        <li>hoid_t  - 32-bit unsigned integer (currently) used as ID for 
            disk-based objects</li>
        <li>hbool_t - boolean to indicate true/false/error codes from functions</li>
        <li>herr_t  - 32-bit integer to indicate succeed/fail codes from 
            functions</li>
    </ul></dd>
<br />
-->
<!-- PRINT VERSION CONTINUED --
<dt><strong><a name="Glossary-LayoutContig">contiguous layout</a></strong></dt>
<dd>The storage layout of a dataset that is not chunked, so that the entire
    data portion of the dataset is stored in a single contiguous block.</dd>
<br />

<dt><b><a name="Glossary-PListDataTransfer">data transfer property list</a></b></dt>
<dd>The data transfer property list is used to control various aspects 
    of the I/O, such as caching hints or collective I/O information.</dd>
<br />

<dt><b><a name="Glossary-Dataset">dataset</a></b></dt>
<dd>A multi-dimensional array of data elements, together with 
    supporting metadata. </dd>
<br />


<dt><b><a name="Glossary-PListDSetAccess">dataset access property list</a></b></dt>
<dd>A property list containing information on how a dataset is to be accessed.</dd>
<br />

<dt><b><a name="Glossary-PListDSetCreate">dataset creation property list</a></b></dt>
<dd>A property list containing information on how 
    raw data is organized on disk and how the raw data is compressed.</dd>
<!--
    The dataset API partitions these terms by layout, compression,
    and external storage:
    <ul>
    <b> Layout:</b>
    <ul>
    <li>H5D_COMPACT: Data is small and can be stored in object header (not
        implemented yet).  This eliminates disk seek/read requests.</li>
    <li>H5D_CONTIGUOUS: (<b>default</b>) The data is large, non-extendible, 
        non-compressible, non-sparse, and can be stored externally.</li>
    <li>H5D_CHUNKED:  The data is large and can be extended in any dimension.
        It is partitioned into chunks so each chunk is the same logical size. </li>
    </ul>
    <b>Compression:</b>  (gzip compression)<br />
    <b>External Storage Properties:</b>  The data must be contiguous to be 
       stored externally. It allows you to store 
       the data in one or more non-HDF5 files.
    </ul>
-->
<!-- PRINT VERSION CONTINUED --
<br />

<dt><b><a name="Glossary-Dataspace">dataspace</a></b></dt>
<dd>An object that describes the dimensionality of the data array. 
    A dataspace is either a regular N-dimensional array of data points, 
    called a simple dataspace, or a more general collection of data points 
    organized in another manner, called a complex dataspace.</dd>
<br />
<!-- NEW PAGE -->
<!-- PRINT VERSION CONTINUED --
<dt><b><a name="Glossary-Datatype">datatype</a></b></dt>
<dd>An object that describes the storage format of the individual data 
    points of a data set.
    There are two categories of datatypes: atomic and compound datatypes.
    An atomic type is a type which cannot be decomposed into smaller 
    units at the API level. A compound datatype is a collection of one or 
    more atomic types or small arrays of such types.</dd>
<br />

<!--
<dt><b>DDL</b></dt>
<dd>A detailed description of the HDF5 format and objects, written in a 
    Data Description Language using Backus-Naur Form.
<br />
-->

<!--
<dt><strong><a name="Glossary-DiskIO">disk I/O datatypes</a></strong></dt>
<ul>
    <li>hoff_t  - (64-bit?) offset on disk in bytes</li>
    <li>hlen_t  - (64-bit?) length on disk in bytes</li>
</ul>
<br />
-->
<!-- PRINT VERSION CONTINUED --
<dt><strong><a name="Glossary-DTypeEnum">enumeration datatype</a></strong></dt>
<dd>A one-to-one mapping between a set of symbols and a set of 
    integer values, and an order is imposed on the symbols by their 
    integer values. The symbols are passed between the application 
    and library as character strings and all the values for a 
    particular enumeration datatype are of the same integer type, 
    which is not necessarily a native type.</dd>
<br />

<dt><b><a name="Glossary-File">file</a></b></dt>
<dd>A container for storing grouped collections of 
    multi-dimensional arrays containing scientific data. </dd>
<br />

<dt><b><a name="Glossary-FileAccessMode">file access mode</a></b></dt>
<dd>Determines whether an existing file will be overwritten,
    opened for read-only access, or opened for read/write access.  
    All newly created files are opened for both reading and
    writing.  </dd>
<!--
    Possible values are:
    <PRE>
      H5F_ACC_RDWR:   Allow read and write access to file. 
      H5F_ACC_RDONLY: Allow read-only access to file. 
      H5F_ACC_TRUNC:  Truncate file, if it already exists, erasing all data 
                      previously stored in the file. 
      H5F_ACC_EXCL:   Fail if file already exists. 
      H5F_ACC_DEBUG:  Print debug information. 
      H5P_DEFAULT:    Apply default file access and creation properties. 
    </PRE>
-->
<!-- PRINT VERSION CONTINUED --
<br />

<dt><b><a name="Glossary-PListFileAccess">file access property list</a></b></dt>
<dd>File access property lists are used to control different methods 
    of performing I/O on files.</dd>
<!--
    <ul>
    <b>Unbuffered I/O:</b> Local permanent files can be accessed with the 
       functions described in Section 2 of the Posix manual, namely open(), 
       lseek(), read(), write(), and close(). <br />
    <b>Buffered I/O:</b> Local permanent files can be accessed with the 
       functions declared in the stdio.h header file, namely fopen(), 
       fseek(), fread(), fwrite(), and fclose().<br />
    <b>Memory I/O:</b> Local temporary files can be created and accessed 
       directly from memory without ever creating permanent storage. 
       The library uses malloc() and free() to create storage space for the 
       file<br />
    <b>Parallel Files using MPI I/O:</b> This driver allows parallel access 
       to a file through the MPI I/O library. The parameters which can be 
       modified are the MPI communicator, the info object, and the access mode.
       The communicator and info object are saved and then passed to 
       MPI_File_open() during file creation or open. The access_mode 
       controls the kind of parallel access the application intends.<br /> 
    <b>Data Alignment:</b> Sometimes file access is faster if certain things 
       are aligned on file blocks. This can be controlled by setting alignment
       properties of a file access property list with the H5Pset_alignment() 
       function. 
    </ul>
-->
<!-- PRINT VERSION CONTINUED --
<br />

<dt><b><a name="Glossary-PListFileCreate">file creation property list</a></b></dt>
<dd>The property list used to control file metadata. </dd>
<!--
    The parameters that can be modified are:
    <ul>
    <b>User-Block Size:</b> The "user-block" is a fixed length block 
       of data located at the beginning of the file which is ignored 
       by the HDF5 library and may be used to store any data information 
       found to be useful to applications. 
    <br />
    <b>Offset and Length Sizes:</b> The number of bytes used to store the
       offset and length of objects in the HDF5 file can be controlled 
       with this parameter. 
    <br />
    <b>Symbol Table Parameters:</b> The size of symbol table B-trees can 
       be controlled by setting the 1/2 rank and 1/2 node size 
       parameters of the B-tree. 
    <br />
    <b>Indexed Storage Parameters:</b> The size of indexed storage 
       B-trees can be controlled by setting the 1/2 rank and 1/2 node 
       size parameters of the B-tree.
    </ul>
-->
<!-- PRINT VERSION CONTINUED --
<br />

<dt><b><a name="Glossary-Group">group</a></b></dt>
<dd>A structure containing zero or more HDF5 objects, 
    together with supporting metadata. 
    The two primary HDF5 objects are datasets and groups.</dd>
<br />

<dt><strong><a name="Glossary-LinkHard">hard link</a></strong></dt>
<dd>A direct association between a name and the object where both exist 
    in a single HDF5 address space.</dd>
<br />

<!--
<dt><b>HDF5</b> </dt>
<dd>HDF5 is an abbreviation for Hierarchical Data Format Version 5. 
    This file format is intended to make it easy to write and read 
    scientific data
    <br />
    <ul>
    <li>by including the information needed to understand the data 
        within the file</li>
    <br />
    <li>by providing a library of C, FORTRAN, and other language 
        programs that reduce the work required to provide efficient 
        writing and reading - even with parallel IO</li>
    </ul></dd>
<br />
-->
<!-- PRINT VERSION CONTINUED --

<dt><b><a name="Glossary-Hyperslab">hyperslab</a></b></dt>
<dd>A portion of a dataset. A hyperslab selection can be a 
    logically contiguous collection of points in a dataspace or
    a regular pattern of points or blocks in a dataspace. </dd>
<br />

<dt><strong><a name="Glossary-Identifier">identifier</a></strong></dt>
<dd>A unique entity provided by the HDF5 library and used to access
    an HDF5 object such as a file, group, or dataset. In the past, 
    an identifier might have been called a handle.</dd>
<br />

<dt><strong><a name="Glossary-Link">link</a></strong></dt>
<dd>An association between a name and the object in an HDF5 file group.</dd>
<br />

<dt><strong><a name="Glossary-GroupMember">member</a></strong></dt>
<dd>A group or dataset that is in another dataset, <i>dataset A</i>,
    is a member of <i>dataset A</i>.</dd>
<br />

<dt><b><a name="Glossary-Name">name</a></b></dt>
<dd>A slash-separated list of components that uniquely identifies an 
    element of an HDF5 file.  A name begins that begins with a slash 
    is an absolute name which is accessed beginning with the root group 
    of the file; all other names are relative names and the associated
    objects are accessed beginning with the current or specified group.</dd>
<br />

<dt><strong><a name="Glossary-DTypeOpaque">opaque datatype</a></strong></dt>
<dd>A mechanism for describing data which cannot be otherwise described 
    by HDF5. The only properties associated with opaque types are a 
    size in bytes and an ASCII tag.</dd>
<br />

<!--
<dt><b>parallel I/O HDF5</b></dt>
<dd>The parallel I/O version of HDF5 supports parallel file access using 
    MPI (Message Passing Interface).  </dd>
<br /> 
-->
<!-- PRINT VERSION CONTINUED --
<dt><strong><a name="Glossary-Path">path</a></strong></dt>
<dd>The slash-separated list of components that forms the name 
    uniquely identifying an element of an HDF5 file.</dd>
<br />

<dt><strong><a name="Glossary-PList">property list</a></strong></dt>
<dd>A collection of name/value pairs that can be passed to other 
    HDF5 functions to control features that are typically unimportant 
    or whose default values are usually used. </dd>
<br />

<dt><strong><a name="Glossary-RootGroup">root group</a></strong></dt>
<dd>The group that is the entry point to the group graph in an HDF5 file.
    Every HDF5 file has exactly one root group.</dd>
<br />

<dt><strong><a name="Glossary-Selection">selection</a></strong></dt>
<dd>(1) A subset of a dataset or a dataspace, up to the entire dataset or 
    dataspace.
    (2) The elements of an array or dataset that are marked for I/O.</dd>
<br />

<dt><strong><a name="Glossary-Serialization">serialization</a></strong></dt>
<dd>The flattening of an <em>N</em>-dimensional data object into a 
    1-dimensional object so that, for example, the data object can be 
    transmitted over the network as a 1-dimensional bitstream.</dd>
<br />

<dt><strong><a name="Glossary-LinkSoft">soft link</a></strong></dt>
<dd>An indirect association between a name and an object in an 
    HDF5 file group.</dd>
<br />

<dt><strong><a name="Glossary-StorageLayout">storage layout</a></strong></dt>
<dd>The manner in which a dataset is stored, either contiguous or
    chunked, in the HDF5 file.</dd>
<br />

<dt><b><a name="Glossary-SuperBlock">super block</a></b></dt>
<dd>A block of data containing the information required to portably access 
    HDF5 files on multiple platforms, followed by information about the groups 
    and datasets in the file. 
    The super block contains information about the size of offsets, 
    lengths of objects, the number of entries in group tables,
    and additional version information for the file. </dd>
<br />

<!--
<dt><b>threadsafe</b></dt>
<dd>A "thread-safe" version of HDF-5 (TSHDF5) is one that can be called 
    from any thread of a multi-threaded program. Any calls to HDF
    can be made in any order, and each individual HDF call will perform 
    correctly. A calling program does not have to explicitly lock the HDF
    library in order to do I/O. Applications programmers may assume that 
    the TSHDF5 guarantees the following: 
    <ul>
        <li>the HDF-5 library does not create or destroy threads. </li>
        <li>the HDF-5 library uses modest amounts of per-thread 
          private memory. </li>
        <li>the HDF-5 library only locks/unlocks it's own locks (no locks 
          are passed in or returned from HDF), and the internal locking 
          is guaranteed to be deadlock free. </li>
    </ul>
    <br />
    These properties mean that the TSHDF5 library will not interfere 
    with an application's use of threads. A TSHDF5 library is the same
    library as regular HDF-5 library, with additional code to synchronize 
    access to the HDF-5 library's internal data structures. </dd>
<br />
-->
<!-- PRINT VERSION CONTINUED --
<dt><strong><a name="Glossary-DTypeVLen">variable-length datatype</a></strong></dt>
<dd>A sequence of an existing datatype (atomic, variable-length (VL), 
    or compound) which are not fixed in length from one dataset location 
    to another.</dd>
<br />

</dl>

<!-- FOR USE WITH PRINT VERSION -------------------------------------->

</body>
</html>

