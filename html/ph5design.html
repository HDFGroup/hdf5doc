<HTML>
<HEAD>
<META HTTP-EQUIV="Content-Type" CONTENT="text/html; charset=windows-1252">
<META NAME="Generator" CONTENT="Microsoft Word 97">
<TITLE>Parallel HDF5 -- Design</TITLE>
</HEAD>
<BODY bgcolor="#FFFFFF">


<hr>
<center>
<table border=0 width=98%>
<tr><td valign=top align=left>
<a href="H5.intro.html">Introduction to HDF5</a>&nbsp;<br>
<a href="H5.user.html">HDF5 User Guide</a>&nbsp;<br>
<a href="RM_H5Front.html">HDF5 Reference Manual</a>&nbsp;<br>
<a href="index.html">Other HDF5 documents and links</a>&nbsp;<br>
<!--
<a href="Glossary.html">Glossary</a><br>
-->
</td>
<td valign=top align=right>
Parallel HDF5 Documents:<br>
<a href="ph5design.html">Parallel HDF5 Design</a><br>
<a href="ph5implement.html">Implementation Release Notes</a></td>
</tr>
</table>
</center>
<hr>




<B><FONT SIZE=6><P ALIGN="CENTER">Parallel HDF5 Design</P>
</B></FONT><P ALIGN="CENTER">&nbsp;</P>
<H1>1. Design Overview</H1>
<P>This section describes the function requirements of the Parallel HDF5 (PHDF5) software and the assumed system requirements. Section 2 describes the programming model of the PHDF5 interface. Section 3 shows an example PHDF5 program. </P>
<H2>1.1. Function requirements</H2>

<UL>
<LI>An API to support parallel file access for HDF5 files in a message-passing environment.   Other types of parallel computing environments such as shared memory may be added in future versions</LI>
<LI>Fast parallel I/O to large datasets through standard parallel I/O interface.   The standard interface is important since the library needs to work in different platforms.</LI>
<LI>No user process can be set aside for I/O purpose only such as an I/O requests daemon. The user application has full use of all processes all the time.  No threads facility can be used either since it is not standard in all platforms.</LI>
<LI>Processes are required to do collective API calls only when structural changes (e.g., file open, dataset definition) occur to the file. (Collective in this case means all processes accessing the file must all make the same API when structural changes occur.)</LI>
<LI>Each process may do independent I/O requests of different datasets in the same or different HDF5 files. </LI>
<LI>Supports collective I/O requests for datasets. (Collective in this context means the processes that have opened the file all make the same I/O request call though may each use different argument values.  This is often used when the processes are accessing the file in small fragmented requests.  The collective call may be able to combined fragmented requests from multiple processes into fewer but bigger I/O requests, thus providing better I/O performance.)  </LI>
<LI>Minimize deviation from the serial HDF5 interface. </LI>
<LI>HDF5 files created with the PHDF5 interface must be totally readable by the serial HDF5 interface and vice versa.  (It may require system utilities to convert HDF5 files when they transfer between the parallel and sequential file systems.)</LI>
<LI>Initial supported platforms include the three ASCI platforms--IBM SP2, Intel TFLOPS and SGI Origin 2000. </LI></UL>

<P>&nbsp;</P>
<H2>1.2. Design Specification</H2>

<UL>
<LI>Choose Message Passing Interface (MPI) for interprocess communication and the MPI-IO API as defined in MPI for parallel I/O accesses.  The MPI library is a commonly accepted standard among parallel computing environment.  The vendors for the ASCI platforms have committed to support the MPI library.  Nearly all Unix platforms as well as Microsoft Windows platforms, MPI is supported either by the vendors or some public domain software.</LI>
<LI>C language interface is the initial requirement. Other interface such as Fortran 90 may be added later. </LI>
<LI>In order for the parallel API's to deviate from the serial API's, we choose not to define a different set of functions for parallel API purpose only.  Instead, all parallel access requests are specified via the property list arguments.  The property list design, besides providing a similar API for both current serial and parallel HDF5 interfaces, also permits future extension of HDF5 support for other types of parallel computing environments such as share memory systems.</LI></UL>

<H1>2. Programming Model</H1>
<P>PHDF5 supports parallel access to HDF5 files in the MPI environment. MPI is a standard interface for the distributed memory parallel computing environment in which inter-process communication is done by message passing. The MPI standard documents are available at <A HREF="http://www.mpi-forum.org/">http://www.mpi-forum.org</A>. Other related MPI information such as tutorials and implementations can be found at <A HREF="http://www.mcs.anl.gov/Projects/mpi/">http://www.mcs.anl.gov/Projects/mpi/</A>.The following describes the programming model of HDF5 specific to the MPI environment.  For a general and more complete understanding of the HDF5 library, one may consult the documents and user guide at <A HREF="http://hdf.ncsa.uiuc.edu/HDF5">http://hdf.ncsa.uiuc.edu/HDF5</A>.</P>
<P>&nbsp;</P>
<P>HDF5 uses property lists to control the file access mechanism. The general model in accessing an HDF5 file in parallel contains the following steps: </P>

<UL>
<LI>Setup access property list for parallel access</LI>
<LI>File create/open </LI>
<LI>Dataset create/open </LI>
<LI>Dataset extension (when needed) </LI>
<LI>Dataset data access with appropriate data transfer property list </LI>
<LI>Dataset close </LI>
<LI>File close </LI></UL>

<H2>2.1. Setup access property list</H2>
<P>Each process of the MPI communicator creates an access property list via H5Pset_mpi and sets it up with MPI information (communicator, info object) as required by the MPI_File_open as defined in MPI-2. Note that H5Pset_mpi does not make duplicates of the communicator or the info object. PHDF5 library will make duplicates of them when an HDF5 file is opened. Therefore, any changes to the communicator or info object will affect the H5Fcreate/H5Fopen calls following the changes. Users are advised not to make changes to the communicator or the info object after the H5Pset_mpi call.</P>
<P>(From this point on, <I>processes </I>are limited to those that are members of the communicator defined in the H5Pset_mpi call.)</P>
<P>&nbsp;</P>
<P>Example:</P>
<FONT FACE="Courier New" SIZE=2><P>/* setup file access property list with parallel IO access. */</P>
<P>acc_pl = H5Pcreate (H5P_FILE_ACCESS);</P>
<P>H5Pset_mpi(acc_pl, comm, info);</P>
<P>&nbsp;</P>
</FONT><P>&nbsp;</P>
<H2>2.1. File create/open</H2>
<P>All processes of the MPI communicator open an HDF5 file by a collective call (H5FCreate or H5Fopen) with the access property list. The call must be collective because the underlying MPI_File_open() is a collective call.</P>
<P>Example:</P>
<FONT FACE="Courier New" SIZE=2><P>/* create the file collectively */</P>
<P>fid=H5Fcreate("filexyz",H5F_ACC_TRUNC,H5P_DEFAULT,acc_pl);</P>
</FONT><P>&nbsp;</P>
<H2>2.2. Dataset create/open</H2>
<P>All processes of the MPI communicator open a dataset by a collective call (H5Dcreate or H5Dopen).&nbsp; This version supports only collective dataset open.&nbsp; The call must be collective because all processes need to have a common knowledge of this dataset object is being accessed.  This allows cooperative changes to the dataset object later.  Future version may support datasets open by a subset of the processes that have opened the file. </P>
<P>Example:</P>
<FONT FACE="Courier New" SIZE=2><P>/* create a 512x1024 dataset */</P>
<P>hsize_t dims[2] = {512, 1024};</P>
<P>sid = H5Screate_simple (2, dims, NULL);</P>
<P>dataset = H5Dcreate(fid, "dataset1", H5T_NATIVE_INT, sid, H5P_DEFAULT);</P>
</FONT><P>&nbsp;</P>
<H2>2.3. Dataset access</H2>
<H3>2.3.1. Independent dataset access</H3>
<P>Each process may do independent and arbitrary number of data I/O accesses by independent calls (H5Dread or H5Dwrite) to the dataset with the transfer property list set for independent access.&nbsp; (The default transfer mode is independent transfer).&nbsp; </P>
<P>If the dataset is an unlimited dimension one and if the H5Dwrite is writing data beyond the current dimension size of the dataset, all processes that have opened the dataset must make a collective call (H5Dallocate) to allocate more space for the dataset BEFORE the independent H5Dwrite call. The reason is that when data is written to beyond the current dimension size, it must be increased to hold the new data.  Changing the dimension size of a dataset is a structural change of the object and must be done by all processes.</P>
<H3>2.3.2. Collective dataset access</H3>
<P>All processes that have opened the dataset may do collective data I/O access by collective calls (H5Dread or H5Dwrite) to the dataset with the transfer property list set for collective access.&nbsp; Pre-allocation (H5Dallocate) is not needed for unlimited dimension datasets since the H5Dallocate call, if needed, is done internally by the collective data access call.  Though all collective accesses can be replaced with independent accesses by each process, collective accesses can provide a better performance if the equivalent independent accesses result in small fragments.[Rajeev’s paper]   A simple example is that a two dimensional dataset is stored in row major order.  When each process needs to access the data by columns, individual independent access by each process would result in multiple uncoordinated access to the dataset with each access segment of the size of the column width.  But if all processes can access the dataset at the same with one collective call, the library, with the extra information of the access pattern, can combine the small accesses into bigger I/O accesses and use gather/scatter to transfer data between all processes.</P>
<P>&nbsp;</P>
<H3>2.3.3. Dataset attributes access</H3>
<P>Changes to attributes can only occur at the <I>"main process" </I>(process 0).&nbsp; Read only access to attributes can occur independent in each process that has opened the dataset.&nbsp;&nbsp; </P>
<H2>2.4. Dataset close</H2>
<P>All processes that have opened the dataset must close the dataset by a collective call (H5Dclose).   The call must be collective so that all processes would have the same knowledge that the dataset is no longer being accessed.</P>
<H2>2.5. File close</H2>
<P>All processes that have opened the file must close the file by a collective call (H5Fclose). The call must be collective because the underlying MPI_File_close() is a collective call.</P>
<P>&nbsp; </P>
<H1>3. Parallel HDF5 Example</H1>
<P>The following are example codes of using the parallel HDF5 API.  The <A HREF="ph5eg_main.c">main program</A> and the <A HREF="ph5eg_testphdf5.h">testphdf5.h</A> files can be viewed at these links.</P>
<H2>3.1. Opening multiple HDF5 files with different communicators</H2>
<P>This example shows how to open two HDF5 files with two different communicators containing two groups of processes.</P>
<P><A HREF="ph5eg_comm.c">Example: Multi-open</A></P>
<H2>3.2. Accessing a dataset via independent transfer mode</H2>
<P>This example shows how to create a fixed dimension dataset and then each process writes and reads data to and from part of the dataset independent of other processes.</P>
<P><A HREF="ph5eg_indep.c">Example: Independent access</A></P>
<H2>3.3. Accessing a dataset via collective transfer mode</H2>
<P>This example shows how to create a fixed dimension dataset and then all processes write and read data to and from the dataset in the collective mode.</P>
<P><A HREF="ph5eg_coll.c">Example: Collective access</A></P>
<H2>3.4. Accessing an extendible dimension dataset</H2>
<P>This example shows how to create an extendible dimension dataset and then all processes collective extend the size of the dataset.  Then each process writes and reads data to and from part of the dataset independent of other processes.</P>
<P><A HREF="ph5eg_extindep.c">Example: Independent access to extendible</A></P>

<P>
<HR>
Comments and questions:
<A HREF="mailto:hdfparallel@ncsa.uiuc.edu">hdfparallel@ncsa.uiuc.edu</A>
<br>
Last modified: 28 Dec 1998
</BODY>
</HTML>
